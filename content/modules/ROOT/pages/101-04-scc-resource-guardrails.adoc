= Lab: SCC Limits & Resource Guardrails
:labid: LAB-101-04
:cis-summary: "Restrict granting of privileged SCCs like anyuid—prefer fixing images to run under restricted defaults. Set default container requests/limits and namespace quotas to cap usage and prevent noisy neighbors."
:mitre-summary: "Prevents container privilege escalation by denying root/privileged assumptions and discouraging anyuid exceptions. Prevents resource exhaustion and noisy-neighbor denial by enforcing default requests and namespace quotas."
:audit-evidence: "Port 80 start and runAsUser=0 fail under restricted; after anyuid grant pod runs as UID 0 illustrating risky exception. LimitRange injects default requests/limits; quota denies excessive replicas and oversized limits with observable FailedCreate/quota events."
:cis-mitre-codes: '{"cisMapping":{"primary":["5.2.6","4.2.8"],"related":[]},"mitre":{"techniques":["T1611","T1499"],"tactics":["TA0004","TA0040"],"mitigations":["M1048","M1026","M1037"]}}'
:toc:
:sectnums:
:icons: font

== Skill
Learn that third-party applications (like vendor or legacy software) often can't be rebuilt or modified to fit OpenShift's security requirements, such as running without root privileges or binding to privileged ports. In these real-world scenarios, the key skill is to analyze the specific security risks and make an informed decision to accept those risks, rather than automatically weakening platform security by granting broad permissions like anyuid. You'll diagnose why such an image fails on OpenShift, understand the security implications, and learn responsible risk management with proper documentation.

Additionally, master the ability to enforce resource controls in OpenShift by setting per-container defaults and namespace-wide ceilings. This ensures fair resource distribution, prevents noisy neighbors from disrupting other workloads, and avoids unexpected cost spikes. You'll learn to use LimitRange for container-level defaults and ResourceQuota for namespace-level boundaries, creating a predictable and balanced multi-tenant environment.

== Objective

* Attempt to run a container binding privileged port 80 and observe failure
* Attempt to force root (runAsUser=0) and see SCC block it
* Grant anyuid and observe the container now runs as root on port 80
* Explain why this is insecure and when the proper alternative is to analyze and accept the risk (for third-party apps where rebuilding is not feasible)
* Apply a LimitRange (defaults + min/max + maxLimitRequestRatio)
* Apply a ResourceQuota (namespace aggregate ceilings)
* Deploy a pod with no resources and verify injected defaults
* Override resources within bounds (allowed)
* Attempt to exceed max (admission rejection)
* Attempt to scale beyond quota (quota denial event)
* Read events / jsonpath outputs to confirm enforcement

== Why it Matters
Using an image that insists on a fixed user or root is like a touring band demanding to rewire your venue's electrical system before playing. You could say yes (faster "win"), but you inherit fire risk, insurance issues, and future lock-in. Granting anyuid is the platform equivalent: a convenience that normalizes bypassing guardrails.

OpenShift injects a random non-root UID so containers can't rely on root. Images built without flexible ownership (root:0 with group write) break, flushing out insecure design early. The business upside: fewer "surprise" escalations, simpler compliance ("all workloads non-root by default"), and faster production readiness.

Without resource controls, containers can request unlimited CPU and memory, much like an all-you-can-eat buffet with no portion control. This leads to "noisy neighbor" problems where one application starves others of resources, causing unexpected workload evictions and runaway cloud costs.

In shared OpenShift environments, this chaos is prevented through two complementary mechanisms: LimitRange sets sensible defaults for individual containers (ensuring every workload has minimum guarantees and maximum caps), while ResourceQuota establishes namespace-wide boundaries (preventing any single team from monopolizing cluster capacity). Together, they create predictable multi-tenancy, fair resource distribution, and early warning signals when capacity limits are approaching—turning potential resource conflicts into manageable, observable events.

Business value: Resource controls translate to cost predictability, performance stability, and tenant isolation. Without them, one misconfigured deployment can cascade into cluster-wide degradation and emergency spending.

NOTE: Permission Requirements for This Lab
Most steps require only project-level access. Granting the `anyuid` SCC needs cluster-admin (or delegated). LimitRange & ResourceQuota creation usually needs elevated (cluster-admin or delegated) rights. Forbidden errors are expected guardrails.

== Concepts: SCC, LimitRange, and ResourceQuota

*Security Context Constraints (SCCs)* control what security-sensitive settings a pod can use (e.g., runAsUser, privileged, hostPath). The *restricted* SCC is the default for most workloads: it assigns a random non-root UID and blocks privileged ports and elevated capabilities. When an image assumes root or port 80, the pod fails—which is desirable, because it surfaces insecure assumptions early. *anyuid* is an SCC that allows running as any user (including root); granting it bypasses that protection and is only acceptable when you have analyzed the risk and documented the exception (e.g., third-party vendor apps that cannot be rebuilt).

*LimitRange* defines default and min/max resource values for containers in a namespace. Pods with no resource spec get the defaults; pods that request more than the max or violate the maxLimitRequestRatio are rejected at admission. That gives every workload predictable defaults and prevents a single container from claiming unbounded CPU or memory.

*ResourceQuota* sets namespace-wide ceilings (e.g., total CPU requests, total memory limits, number of pods). When the sum of resource usage in the namespace would exceed the quota, new pods are not scheduled and you see FailedCreate events. Together with LimitRange, quotas enable fair multi-tenancy and early warning when a namespace is approaching capacity.

== What it Solves
* Eliminates silent privilege creep (running as root "just because")
* Discourages shipping rigid vendor images with hard-coded UIDs
* Prevents broad SCC sprawl (exception fatigue)
* Forces reusable, multi-tenant safe image patterns
* Eliminates silent resource starvation
* Stops accidental over-scaling
* Provides cost & capacity signal (events)
* Sets baseline for autoscaling fairness

Attackers look for root-running containers or SCC exceptions. By refusing them upfront, you reduce accessible pathways before an exploit even runs.

== Understanding the Attack Surface
[cols="1,2,2",options="header"]
|===
|Area | Risk If Ignored | Analogy
|Privileged Port (80) Requirement | Forces elevation; encourages anyuid grant | Band demands rewiring venue power
|Hard-Coded Root Expectation | Normalizes high-privilege baseline | Always giving guests master keys
|Fixed UID Ownership in Image | Leads to permission hacks / SCC exceptions | Furniture bolted for one resident
|Granting anyuid Broadly | Expands blast radius for compromise | Removing door locks for convenience
|Missing Writable Group Ownership | Ops weaken guardrails to "make it work" | Disabling smoke alarm to stop beeping
|Running as Root + Writable FS | Host / kernel probing path | Car left idling with doors unlocked
|Lack of Audit on SCC Exceptions | Drift accumulates | Untracked temporary badges pile up
|No Education on High Ports | Repeated privileged port asks | Insisting on vault door, not side entrance
|No Defaults | Unbounded grabs, neighbor starvation | Open buffet no portions
|Inflated Limits vs Requests | Illusory capacity, waste | Reserved empty parking spots
|Missing ResourceQuota | Single tenant monopolizes | One team books all rooms
|Understated Requests | Over-density → throttling | Small room for big workshop
|Overstated Requests | Fragmentation & waste | 20-seat room for 2 people
|Ignoring Quota Events | Late saturation discovery | Ignoring low-fuel light
|No Ratio Guard | Spiky bursts unfair | Hogging 5 treadmills
|===

== How to Secure (Lifecycle View)
* Build: Ensure writable dirs are group-writable; avoid USER root in final stage. Annotate images with sizing guidance.
* Registry: Store only images passing a non-root readiness check. Store performance baseline docs.
* Deploy: For third‑party apps, analyze risks and register acceptance with annotations; rely on restricted SCC otherwise. Enforce LimitRange early; quotas per env.
* Runtime: RHACS policy flags root or privileged UID usage. Monitor denial events as leading signals.

== Part A: SCC Limits & Non-Root Realities

You will deploy an image that tries to bind to privileged port 80 and run as root, observe failures under the restricted SCC, then (with cluster-admin) grant anyuid to see the anti-pattern, and finally register risk acceptance with namespace annotations so exceptions are auditable. The goal is to understand why guardrails exist and how to document exceptions when fixing the image is not an option.

=== Use case A1: Create project and deploy (attempting port 80)
Create a sandbox project and deploy a UBI Python image that starts an HTTP server on port 80. Under the restricted SCC, the container runs with a random non-root UID and cannot bind to port 80, so the pod will crash.

*Procedure*

. Create the project:

[source,sh]
----
oc new-project 101-04-s-scc-demo
----

. Deploy the app with a command that binds to port 80:

[source,sh]
----
oc create deployment vendor-app --image=registry.access.redhat.com/ubi9/python-311 -- python3 -m http.server 80 -n 101-04-s-scc-demo
----

. Wait a few seconds, then check pod status and logs:

[source,sh]
----
oc get pods -l app=vendor-app -n 101-04-s-scc-demo
oc logs -l app=vendor-app --tail=50 -n 101-04-s-scc-demo
----

*Expected:* Pod in `CrashLoopBackOff` or `Error`. Logs show a permission error binding to port 80, for example:

[source,text]
----
PermissionError: [Errno 13] Permission denied
----

TIP: The correct fix is to change the app to listen on a high port (e.g., 8080). Do not grant anyuid just to satisfy port 80.

=== Use case A2: Attempt to force root (runAsUser=0) – expect denial
Patch the deployment to set `runAsUser: 0` (root). The restricted SCC does not allow UID 0, so the admission controller or SCC logic will reject the pod or you will see security-related events.

*Procedure*

. Patch the deployment to add a securityContext with runAsUser 0:

[source,sh]
----
oc patch deployment vendor-app -n 101-04-s-scc-demo --type='json' -p='[{"op":"add","path":"/spec/template/spec/securityContext","value":{"runAsUser":0}}]'
----

. Restart the deployment and check events:

[source,sh]
----
oc rollout restart deployment/vendor-app -n 101-04-s-scc-demo
oc get events -n 101-04-s-scc-demo --sort-by=.lastTimestamp | grep vendor-app | tail -n 10
----

*Expected:* Look for messages indicating invalid UID range, runAsUser not allowed, or similar SCC/security context rejection. The pod may remain in a failed state.

IMPORTANT: This step demonstrates that the platform blocks root by default. Do not rely on "we'll patch it later"—fix the image to run as non-root and use a high port.

=== Use case A3: Grant anyuid (anti-pattern, requires cluster-admin)
If you have cluster-admin (or equivalent), you can grant the default service account the anyuid SCC. The pod will then run as root and can bind to port 80. This is shown only to illustrate the risk; in production, avoid broad anyuid grants and prefer image fixes or scoped, documented exceptions.

*Procedure*

. Grant the default service account in the project the anyuid SCC (requires cluster-admin):

[source,sh]
----
oc adm policy add-scc-to-user anyuid -z default -n 101-04-s-scc-demo
----

If you get `Forbidden`, you do not have sufficient rights—that is expected and indicates the guardrail is in place.

. Restart the deployment so the new SCC is applied:

[source,sh]
----
oc rollout restart deployment/vendor-app -n 101-04-s-scc-demo
oc rollout status deployment/vendor-app -n 101-04-s-scc-demo --timeout=120s
----

. Verify the container is running as root and the server is listening on 80:

[source,sh]
----
oc exec deploy/vendor-app -n 101-04-s-scc-demo -- id -u
oc logs -l app=vendor-app --tail=10 -n 101-04-s-scc-demo
----

*Expected (if anyuid was granted):* `id -u` prints `0` and logs show the server started on port 80. This confirms that anyuid bypasses the non-root and privileged-port guardrails—and why it should be avoided except when explicitly accepted.

=== Use case A4: Register risk acceptance (namespace annotation)
For third-party or legacy apps where rebuilding is not feasible, document the exception with namespace (or deployment) annotations so it is discoverable and auditable. Use a consistent key pattern such as `openshift.io/risk-accepted.<deployment-name>`.

*Procedure*

. Annotate the namespace with risk-acceptance metadata for this deployment:

[source,sh]
----
oc annotate namespace 101-04-s-scc-demo \
  openshift.io/risk-accepted.vendor-app="Requires anyuid for privileged port" \
  openshift.io/risk-accepted-approved-by.vendor-app="InfoSec Team" \
  --overwrite
----

. Verify the annotations are present:

[source,sh]
----
oc get ns 101-04-s-scc-demo -o yaml | grep -E 'openshift.io/risk-accepted'
----

*Sample output*

[source,text]
----
  openshift.io/risk-accepted-approved-by.vendor-app: InfoSec Team
  openshift.io/risk-accepted.vendor-app: Requires anyuid for privileged port
----

NOTE: Reserve this pattern for exceptional cases. The preferred path is to rebuild images to run under the restricted SCC without anyuid. For multiple deployments, annotate each (e.g., suffix keys with the deployment name) or annotate the Deployment object itself.

=== Use case A5: Cleanup (Part A)
. Delete the project:

[source,sh]
----
oc delete project 101-04-s-scc-demo --wait=false
----

== Part B: Resource Usage (Quotas & Limits)

You will create a second namespace, apply a LimitRange (defaults, min/max, and maxLimitRequestRatio) and a ResourceQuota (namespace ceilings). Then you will deploy a pod with no resource spec (it inherits defaults), override resources within bounds, attempt to exceed the LimitRange max (admission rejects), and scale beyond the quota (FailedCreate events). All steps use the namespace `101-04-s-resources-demo` and a UBI image running `sleep infinity`.

=== Use case B1: Namespace and LimitRange
Create the resources namespace and apply a LimitRange so every container gets default requests and limits and is constrained by min/max and ratio.

*Procedure*

. Create the project and switch to it:

[source,sh]
----
oc new-project 101-04-s-resources-demo
oc project 101-04-s-resources-demo
----

. Create a LimitRange with defaults, min, max, and maxLimitRequestRatio:

[source,sh]
----
oc apply -f - <<'EOF'
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: 101-04-s-resources-demo
spec:
  limits:
  - type: Container
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    default:
      cpu: 500m
      memory: 256Mi
    min:
      cpu: 50m
      memory: 64Mi
    max:
      cpu: "1"
      memory: 512Mi
    maxLimitRequestRatio:
      cpu: "5"
      memory: "4"
EOF
----

. Verify the LimitRange was created:

[source,sh]
----
oc get limitrange default-limits -n 101-04-s-resources-demo -o yaml
# Or a compact view:
oc get limitrange default-limits -n 101-04-s-resources-demo -o jsonpath='{.spec.limits[0]}' | sed 's/,/\n/g'
----

TIP: The maxLimitRequestRatio prevents a container from requesting a small amount and then using a much larger limit (e.g., request 100m CPU but limit 2 CPU), which can lead to unfair burst usage.

=== Use case B2: ResourceQuota (namespace ceilings)
Apply a ResourceQuota so the namespace cannot exceed aggregate CPU, memory, and pod counts. New pods that would push the namespace over the quota will fail with a quota-related FailedCreate event.

*Procedure*

. Create the quota (single --hard with comma-separated values; requires permission to create resourcequotas):

[source,sh]
----
oc create quota compute-quota -n 101-04-s-resources-demo \
  --hard=requests.cpu=2,requests.memory=2Gi,limits.cpu=4,limits.memory=4Gi,pods=10
----

. Inspect the quota and current usage:

[source,sh]
----
oc describe quota compute-quota -n 101-04-s-resources-demo
----

*Sample output*

[source,text]
----
Name:            compute-quota
Namespace:       101-04-s-resources-demo
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     4
limits.memory    0     4Gi
pods             0     10
requests.cpu     0     2
requests.memory  0     2Gi
----

=== Use case B3: Deploy unbounded workload (inherits defaults)
Deploy a pod that does not specify resources. The LimitRange admission controller injects the default request and limit, so the pod is scheduled with predictable values.

*Procedure*

. Create a deployment with no resource spec:

[source,sh]
----
oc create deployment stress -n 101-04-s-resources-demo --image=registry.access.redhat.com/ubi9/ubi -- /bin/sh -c "sleep infinity"
----

. Wait for the deployment to be available and inspect the pod's injected resources:

[source,sh]
----
oc wait --for=condition=Available deployment/stress -n 101-04-s-resources-demo --timeout=90s
oc get pod -l app=stress -n 101-04-s-resources-demo -o jsonpath='{.items[0].spec.containers[0].resources}{"\n"}'
----

*Expected:* The container has requests (e.g., cpu 100m, memory 128Mi) and limits (e.g., cpu 500m, memory 256Mi) from the LimitRange defaults.

=== Use case B4: Override within bounds
Change the deployment's resource requests and limits to values within the LimitRange min/max. The rollout should succeed and the new pods reflect your overrides.

*Procedure*

. Set requests and limits within the allowed range:

[source,sh]
----
oc set resources deploy/stress -n 101-04-s-resources-demo --requests=cpu=300m,memory=256Mi --limits=cpu=800m,memory=512Mi
oc rollout restart deploy/stress -n 101-04-s-resources-demo
oc wait --for=condition=Available deployment/stress -n 101-04-s-resources-demo --timeout=90s
----

. Confirm the pod spec shows the new values:

[source,sh]
----
oc get pod -l app=stress -n 101-04-s-resources-demo -o jsonpath='{.items[0].spec.containers[0].resources}{"\n"}'
----

NOTE: 800m CPU and 512Mi memory are at the LimitRange max; 300m/256Mi are within the default and ratio. If you exceed max (e.g., limits.cpu=2), the next step demonstrates the rejection.

=== Use case B5: Exceed max (admission rejects new pods)
Set the deployment's limits above the LimitRange max (e.g., cpu: 2, memory: 1Gi). The new replica set will not be able to create pods; you will see FailedCreate events or conditions indicating the pod was rejected (e.g., LimitRange or Forbidden).

*Procedure*

. Set limits above the LimitRange max:

[source,sh]
----
oc set resources deployment/stress -n 101-04-s-resources-demo --limits=cpu=2,memory=1Gi
oc rollout restart deploy/stress -n 101-04-s-resources-demo
----

. Check deployment conditions and events for the rejection reason:

[source,sh]
----
oc get deploy stress -n 101-04-s-resources-demo -o jsonpath='{range .status.conditions[*]}{.type}:{.reason}:{.message}{"\n"}{end}'
oc get events -n 101-04-s-resources-demo --sort-by=.lastTimestamp | tail -15
----

*Expected:* FailedCreate or similar, with a message about exceeding maximum allowed or LimitRange. The existing pod (from the previous spec) may still be running; the new replica set's pods will not be created.

. Revert to a valid spec so you can continue (e.g., back within bounds):

[source,sh]
----
oc set resources deployment/stress -n 101-04-s-resources-demo --requests=cpu=300m,memory=256Mi --limits=cpu=800m,memory=512Mi
oc rollout status deployment/stress -n 101-04-s-resources-demo --timeout=90s
----

=== Use case B6: Scale beyond quota (pods limit)
Scale the deployment to more replicas than the quota allows (e.g., 25 pods when the quota allows 10). Extra pods will not be scheduled and you will see FailedCreate events citing the quota.

*Procedure*

. Scale to a number of replicas that exceeds the quota (e.g., pods=10 in the quota):

[source,sh]
----
oc scale deployment stress -n 101-04-s-resources-demo --replicas=25
----

. Check events and quota usage:

[source,sh]
----
oc get events -n 101-04-s-resources-demo --sort-by=.lastTimestamp | grep -E 'FailedCreate|compute-quota' | tail -5
oc get quota compute-quota -n 101-04-s-resources-demo
oc get pods -n 101-04-s-resources-demo -l app=stress
----

*Expected:* Only up to 10 pods (or whatever the quota allows) are created; the rest show FailedCreate with a reason related to quota (e.g., "exceeded quota"). The quota describe output shows Used at the Hard limit for pods.

IMPORTANT: ResourceQuota and LimitRange work together: LimitRange injects defaults and enforces per-container bounds; ResourceQuota enforces namespace totals. Use both for predictable, fair multi-tenancy.

=== Use case B7: Cleanup (Part B)
. Delete the project:

[source,sh]
----
oc delete project 101-04-s-resources-demo --wait=false
----

== What Would You Do?

You saw how the restricted SCC blocks root and privileged port 80, how anyuid bypasses those guardrails (and why to avoid it), and how to document risk acceptance with annotations. You also enforced resource defaults and ceilings with LimitRange and ResourceQuota. In your own environment, consider:

* How would you decide when to allow an anyuid exception versus requiring an image change or a high-port variant?
* Would you standardize LimitRange and ResourceQuota per namespace (e.g., via GitOps) so every project has predictable defaults and caps?
* How would you monitor quota denial events and saturation trends to plan capacity before users hit hard limits?

== Solutions / Controls

* Restricted SCC dominance
* Controlled anyuid exceptions with review
* Image hardening pipelines (fail on USER root)
* RHACS detection of root / capability drift
* LimitRange: per-container defaults & bounds
* ResourceQuota: namespace aggregate cap
* Monitoring: watch denial events & saturation trends
* Autoscaling: accurate requests enable fair scaling

== Summary

In Part A you observed a container fail on port 80 and when forced to runAsUser 0 under the restricted SCC, then (with cluster-admin) granted anyuid and saw it run as root on port 80—illustrating why anyuid is a last resort. You recorded risk acceptance with namespace annotations for auditability. In Part B you applied a LimitRange for container defaults and bounds, and a ResourceQuota for namespace ceilings; you verified default injection, allowed overrides within bounds, and saw admission reject over-max limits and quota deny excess replicas. Prefer fixing images and using high ports; when exceptions are unavoidable, document them. Use LimitRange and ResourceQuota in every namespace for fair, predictable multi-tenancy.

== Summary Table
[cols="1,2,2",options="header"]
|===
|Issue | Bad Shortcut | Secure Fix
|Privileged port 80 fails | Grant anyuid | Use high port (8080)
|Writable dirs missing | chmod at runtime | Fix ownership during build
|Legacy root-only startup | Persist anyuid exception | Analyze risk and accept with annotations
|Control | Purpose | Outcome
|LimitRange | Container defaults | Predictable scheduling
|ResourceQuota | Namespace boundary | Fair multi-tenancy
|Requests | Scheduler planning | Prevent overcommit illusions
|Limits | Throttle ceiling | Contain noisy processes
|===

== Error Interpretation Cheat Sheet
[cols="1,2,1,2,2",options="header"]
|===
|Phase | Symptom | Source | Meaning | Right Response
|Port 80 start | PermissionError 13 | App log | Non-root blocked on privileged port | Change to high port
|runAsUser=0 patch | SecurityContext warnings | PodSecurity | Harden settings not declared | Informational
|runAsUser=0 patch | FailedCreate UID invalid | SCC | UID 0 outside allowed range | Drop root attempt
|After anyuid | id -u = 0 on 80 | anyuid SCC | Guardrail bypassed | Revert and refactor
|===

== FAQs
Why random UID instead of a fixed one?:: Prevents brittle assumptions; enforces portability.
Is anyuid always bad?:: Not inherently—broad usage is.
Can SELinux cause similar failures?:: Yes; rule out UID/perm design first, then inspect AVC denials.
Why set requests and limits—not just limits?:: Requests drive scheduling placement.
Can limits harm performance?:: Too tight → throttling; measure realistic peaks.
What if workload is bursty?:: Typical usage as request; safe burst upper bound as limit.

== Closing Story
Granting anyuid because an image fails is like removing a seatbelt because it's "too tight." Fix the workload; keep the safety system.

Resource controls are the booking system preventing one team from reserving every room—fairness yields stability.

== Next Step Ideas

* Add CI linting: warn on USER root
* Inventory existing anyuid bindings
* Dashboards: top quota denial reasons
* Script: detect unused pods holding quota
* Pilot vertical pod autoscaler (recommendation mode)

== Comparison (Before vs After Behavior)
[cols="2,1,1,1,2",options="header"]
|===
|Image + Command | Runs As (SCC) | Port | Result | Why
|ubi9/python-311 + http.server 80 | random non-root (restricted) | 80 | Fails | Privileged port
|ubi9/python-311 + http.server 8080 | random non-root (restricted) | 8080 | Succeeds | High port
|(Anti-pattern) same + anyuid | root (anyuid) | 80 | Succeeds | Guardrail removed
|===
