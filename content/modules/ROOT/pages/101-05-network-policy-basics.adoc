= Lab: You Shall Not Pass – Segmenting Pod Traffic with NetworkPolicies
:labid: LAB-101-05
:cis-summary: "Apply NetworkPolicies so namespaces default deny and only intended pod traffic is allowed."
:mitre-summary: "Prevents lateral movement and internal scanning via default deny plus narrowly scoped allow rules."
:audit-evidence: "Default connectivity works; default-deny blocks both clients; allow policy restores access only for labeled allowed client (200 vs timeout)."
:cis-mitre-codes: '{"cisMapping":{"primary":["5.3.2"]},"mitre":{"techniques":["T1046"],"tactics":["TA0007"],"mitigations":["M1030","M1042"]}}'
:toc:
:sectnums:
:icons: font

== Skill
Shrink lateral movement by moving from implicit allow-all networking to explicit allow rules via NetworkPolicies.

== Objective

* Observe default pod-to-pod connectivity
* Enforce namespace-level default deny
* Create targeted allow policy for one client
* Validate blocked vs allowed flows

== Why it Matters
In a flat network where all pods can talk to each other by default, a single compromised container becomes a launching pad for attackers to explore your entire cluster, scanning for databases, probing internal APIs, and pivoting to sensitive services. This "implicit trust" model is like a building where every door is unlocked: once someone gets in one room, they have access to everything.

NetworkPolicies introduce explicit segmentation, interior doors that require intent rather than assumption. By moving from "allow-all by default" to "deny-all with specific exceptions," you dramatically reduce the attack surface. Even if one pod is compromised, it can only reach the services you've explicitly permitted, limiting lateral movement and making incident response clearer ("what could this pod actually reach?"). This approach aligns with zero-trust principles and satisfies compliance requirements for network segmentation.

Business value: Segmentation contains breaches before they spread, reduces the scope of security audits, and demonstrates defense-in-depth to customers and regulators.

== What it Solves

* Prevents every compromised pod becoming a pivot
* Reduces accidental exposure of internal services
* Eases incident scoping (“what could it reach?”)
* Supports regulated segmentation requirements

== Understanding the Attack Surface
[cols="1,2,2",options="header"]
|===
|Surface | Risk When Open | Example Abuse
|East-West Traffic | Lateral movement | Probe creds service from frontend
|Lack of Egress Control | Data exfiltration | Leak secrets externally
|Overbroad Labels | Wide unintended allowance | Label matches many pods
|Missing Default Deny | Hidden gaps persist | Sensitive svc reachable
|===

== How to Secure (Lifecycle View)
* Build: Standardize labels (app, role, tier).
* Registry: Harden images to reduce footholds.
* Deploy: Default deny early; allowlists per service.
* Runtime: RHACS network graph to refine policy.

== Concepts: Default Allow vs Default Deny

By default, pods in a Kubernetes or OpenShift cluster can reach any other pod—there is no network segmentation. A compromised workload can probe internal services, scrape credentials, or pivot laterally. *NetworkPolicies* let you change that: they are namespace-scoped, label-driven rules that restrict *ingress* (who can talk to a pod) and optionally *egress* (where a pod can talk to). The secure pattern is *default deny*: first apply a policy that selects all pods in the namespace and allows no ingress (so all inbound traffic is dropped), then add *allow* policies that permit only specific sources (e.g., pods with a given label). That way only explicitly allowed traffic reaches your services; everything else is blocked. This lab has you observe open connectivity, apply default-deny ingress, then allow only one labeled client so you can see the difference in behavior.

== How it Works (Labels 101)
NetworkPolicies are label-driven and namespace-scoped. You control two sides with labels:

- Target pods: spec.podSelector selects the pods this policy applies to.
  - {} means “all pods in this namespace.”
- Allowed sources (Ingress): spec.ingress[].from lists who may talk to targets.
  - podSelector: pods in the same namespace that match these labels
  - namespaceSelector: pods from namespaces that match these labels
  - ipBlock: allow specific CIDRs (ingress only)
- Direction: policyTypes: [Ingress, Egress] controls which directions are enforced.

Matching semantics you’ll use most:

- AND within matchLabels (all keys must match); OR across list items:
  - Multiple labels under matchLabels must all match.
  - Multiple entries in from or ports act like OR rules.
- Default deny emerges when a pod is selected by any Ingress policy and no rule allows a given flow; the explicit pattern is a “default-deny” policy with podSelector: {} and no from rules.

Tip: For cross-namespace access, add namespace labels (e.g., env=clients) and use namespaceSelector with those labels. See “How to Try It” for YAML examples.

== How to Try It

You will create a project with an API server and two client pods (one labeled allowed, one denied), verify that both can reach the API with no policies, apply a default-deny ingress policy so all inbound traffic is blocked, then add an allow policy so only the labeled "allowed" client can reach the API. You will validate each step with curl from the client pods. All commands use the namespace `101-05-n-netpol-demo`.

=== Use case 1: Create project and baseline pods
Create the project, deploy a simple HTTP API, expose it as a service, and run two client pods with distinct labels so you can later allow one and block the other.

*Procedure*

. Create the project:

[source,sh]
----
oc new-project 101-05-n-netpol-demo
----

. Deploy the API server (Python HTTP server on port 8080) and expose it as a service:

[source,sh]
----
oc create deployment api -n 101-05-n-netpol-demo --image=registry.access.redhat.com/ubi9/python-311 -- python3 -m http.server 8080
oc expose deployment api -n 101-05-n-netpol-demo --port=8080 --target-port=8080
----

. Create two client pods with labels `role=allowed` and `role=denied` (used later by the allow policy):

[source,sh]
----
oc run client-allowed -n 101-05-n-netpol-demo --image=registry.access.redhat.com/ubi9/ubi -l role=allowed -- sleep infinity
oc run client-denied -n 101-05-n-netpol-demo --image=registry.access.redhat.com/ubi9/ubi -l role=denied -- sleep infinity
----

. Wait for pods to be ready, then store the API service ClusterIP for later curl tests:

[source,sh]
----
oc wait --for=condition=Ready pod -l app=api -n 101-05-n-netpol-demo --timeout=90s
oc wait --for=condition=Ready pod -l role=allowed -n 101-05-n-netpol-demo --timeout=60s
oc wait --for=condition=Ready pod -l role=denied -n 101-05-n-netpol-demo --timeout=60s
SVC=$(oc get svc api -n 101-05-n-netpol-demo -o jsonpath='{.spec.clusterIP}')
echo "API ClusterIP: $SVC"
----

TIP: Use a consistent label taxonomy (e.g., `app`, `role`, `tier`) so NetworkPolicy rules stay readable and auditable. Here `role=allowed` and `role=denied` are used only to demonstrate allow vs block.

=== Use case 2: Test open connectivity (both succeed)
With no NetworkPolicies, all pods can reach each other. Curl from both client pods to the API service and confirm HTTP 200.

*Procedure*

. From the allowed client pod, curl the API (replace with the pod name from `oc get pods -l role=allowed -n 101-05-n-netpol-demo` if your pod name has a random suffix):

[source,sh]
----
oc exec -n 101-05-n-netpol-demo $(oc get pod -l role=allowed -n 101-05-n-netpol-demo -o jsonpath='{.items[0].metadata.name}') -- curl -s -o /dev/null -w '%{http_code}\n' http://$SVC:8080
----

*Expected output:* `200`

. From the denied client pod, curl the API:

[source,sh]
----
oc exec -n 101-05-n-netpol-demo $(oc get pod -l role=denied -n 101-05-n-netpol-demo -o jsonpath='{.items[0].metadata.name}') -- curl -s -o /dev/null -w '%{http_code}\n' http://$SVC:8080
----

*Expected output:* `200`

[mermaid]
----
flowchart LR

  subgraph P1["No Network Policies"]
    CA1[client-allowed]
    CD1[client-denied]
    API1[api]
    CA1 -->|200| API1
    CD1 -->|200| API1
  end
----

=== Use case 3: Apply default deny (Ingress)
Apply a NetworkPolicy that selects all pods in the namespace (`podSelector: {}`) and allows no ingress. Every pod is now selected by at least one Ingress policy, so the default behavior becomes deny: no inbound traffic is allowed unless another policy explicitly permits it.

*Procedure*

. Apply the default-deny ingress policy:

[source,sh]
----
oc apply -f - -n 101-05-n-netpol-demo <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF
----

. Verify the policy is in place:

[source,sh]
----
oc get networkpolicy -n 101-05-n-netpol-demo
----

IMPORTANT: Once a pod is selected by any NetworkPolicy that includes Ingress in policyTypes, only traffic explicitly allowed by some policy can reach it. With only this default-deny policy, no ingress is allowed, so both clients will be blocked in the next step.

=== Use case 4: Retest after default deny (expect timeouts)
Curl from both client pods again. Both should fail to reach the API (connection timeout or refused) because the default-deny policy blocks all ingress to the API pod.

*Procedure*

. Curl from the allowed client with a short timeout (expect failure):

[source,sh]
----
oc exec -n 101-05-n-netpol-demo $(oc get pod -l role=allowed -n 101-05-n-netpol-demo -o jsonpath='{.items[0].metadata.name}') -- sh -c "curl -s --max-time 3 -o /dev/null -w '%{http_code}' http://$SVC:8080" || echo TIMEOUT
----

*Expected:* TIMEOUT or non-200 (no connection).

. Curl from the denied client (expect failure):

[source,sh]
----
oc exec -n 101-05-n-netpol-demo $(oc get pod -l role=denied -n 101-05-n-netpol-demo -o jsonpath='{.items[0].metadata.name}') -- sh -c "curl -s --max-time 3 -o /dev/null http://$SVC:8080" || echo TIMEOUT
----

*Expected:* TIMEOUT.

[mermaid]
----
flowchart LR

  subgraph P2["Default Deny Ingress"]
    CA2[client-allowed]
    CD2[client-denied]
    API2[api]
    CA2 -.->|timeout| API2
    CD2 -.->|timeout| API2
  end
----

=== Use case 5: Allow only the approved client
Add a second NetworkPolicy that selects the API pods (`app: api`) and allows ingress only from pods with `role: allowed` on port 8080. The default-deny policy still applies to all pods, but this allow policy adds an exception for the API from the allowed client. The denied client remains blocked.

*Procedure*

. Apply the allow policy:

[source,sh]
----
oc apply -f - -n 101-05-n-netpol-demo <<'EOF'
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-from-allowed
spec:
  podSelector:
    matchLabels:
      app: api
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: allowed
    ports:
    - protocol: TCP
      port: 8080
  policyTypes:
  - Ingress
EOF
----

. List NetworkPolicies to confirm both are present:

[source,sh]
----
oc get networkpolicy -n 101-05-n-netpol-demo
----

NOTE: Multiple policies can apply to the same pod. If any policy allows a given flow, the traffic is permitted. So the API pod is still selected by default-deny-ingress (no ingress), but allow-api-from-allowed adds an explicit allow for pods with `role=allowed` on port 8080.

=== Use case 6: Retest (allowed=200, denied blocked)
Curl from the allowed client (expect 200) and from the denied client (expect timeout or blocked). This validates that segmentation is in effect.

*Procedure*

. From the allowed client (expect 200):

[source,sh]
----
oc exec -n 101-05-n-netpol-demo $(oc get pod -l role=allowed -n 101-05-n-netpol-demo -o jsonpath='{.items[0].metadata.name}') -- curl -s -o /dev/null -w '%{http_code}\n' http://$SVC:8080
----

*Expected output:* `200`

. From the denied client (expect timeout or BLOCKED):

[source,sh]
----
oc exec -n 101-05-n-netpol-demo $(oc get pod -l role=denied -n 101-05-n-netpol-demo -o jsonpath='{.items[0].metadata.name}') -- sh -c "curl -s --max-time 3 -o /dev/null http://$SVC:8080" || echo BLOCKED
----

*Expected:* BLOCKED or TIMEOUT (no 200).

[mermaid]
----
flowchart LR

  subgraph P3["Allow Specific Client"]
    CA3[client-allowed]
    CD3[client-denied]
    API3[api]
    CA3 -->|200| API3
    CD3 -.->|timeout| API3
  end
----

=== Use case 7: Cleanup
*Procedure*

. Delete the project:

[source,sh]
----
oc delete project 101-05-n-netpol-demo --wait=false
----

== What Would You Do?

You observed default allow-all connectivity, applied default-deny ingress so all traffic was blocked, then allowed only the client with `role=allowed` to reach the API. In your own environment, consider:

* How would you design labels (app, role, tier, env) so every namespace has a consistent vocabulary for NetworkPolicy rules?
* Would you start with default deny in every namespace and then add allow rules from observed flows (e.g., using RHACS network graph)?
* How would you test policies before rollout—e.g., a canary namespace with the same policies and synthetic traffic?

== Solutions / Controls

* NetworkPolicies for microsegmentation
* Standard labels as controlled vocabulary
* RHACS network graph for visualization
* Zero trust mindset: default deny + explicit allow

== Summary

You created an API and two client pods, confirmed both could reach the API with no policies, then applied a default-deny ingress policy so all inbound traffic was blocked. After adding an allow policy for pods with `role=allowed`, only that client could reach the API; the other client was blocked. NetworkPolicies are label-driven and namespace-scoped—default deny plus explicit allow rules shrink lateral movement and support zero-trust segmentation. Use consistent labels and refine policies with observed flows (e.g., RHACS network graph).

== Summary Table
[cols="1,2,2",options="header"]
|===
|What to Secure | Risk | Control
|East-West Traffic | Lateral expansion | Default deny + allowlist
|Labeling Consistency | Policy gaps | Enforced taxonomy
|High-Sensitivity Services | Data exposure | Targeted ingress policies
|Drift Over Time | Stale rules | Periodic review + RHACS
|===

== FAQs
Do NetworkPolicies block egress by default?:: Only if egress types specified.
Why start with default deny?:: Reveals necessary connections.
Are they node firewalls?:: Enforced at pod interfaces via CNI.
How to avoid label sprawl mistakes?:: Minimal taxonomy: app, role, tier, env.

== Closing Story
Segmentation converts a wide-open warehouse into purposeful rooms; movement remains—but intentional.

== Next Step Ideas

* Add egress example (allow DNS, block external)
* Bundle policies into a single manifest
* Visualize flows pre/post with RHACS

== Extra: Network Policy Patterns (Reading)
The following reference patterns are adapted from docs/network-policies.adoc. Use them to reason about design choices; keep YAML only in the hands-on section above.

=== Pattern Format
Each pattern includes: Explanation, Use Case, Risk (Why it matters), Implementation Checklist, Quick Validation steps.

=== Pattern: DENY All Non-Whitelisted Traffic to a Namespace
[mermaid]
----
flowchart LR
  subgraph ns_other ["namespace other"]
    Blog[app=blog]
  end
  subgraph ns_default ["namespace default"]
    API[app=api]
    Guest[app=guestbook]
  end
  Blog -.-> Guest
  Blog -. ❌ .-> API
  API -. ❌ .-> Guest
----

Explanation:: Only approved cross-namespace flow (blog -> guestbook) is permitted; other cross or internal flows are blocked.
Use Case:: Multi-tenant cluster; restrict which external namespace may call a frontend.
Why it Matters:: Reduces lateral movement between namespaces.
Implementation Checklist::
* NetworkPolicy selecting protected pods (e.g. guestbook)
* Ingress rules with `from` including `namespaceSelector + podSelector` for allowed source
* Specify ports
* `policyTypes: [Ingress]`

=== Pattern: LIMIT Traffic to an Application
[mermaid]
----
flowchart LR
  Coffee[app=coffeeshop\\nrole=api]
  BookAPI[app=bookstore\\nrole=api]
  BookFE[app=bookstore\\nrole=frontend]
  BookAPI -.-> BookFE
  Coffee -. ❌ .-> BookAPI
----
Explanation:: Frontend (role=frontend) may call bookstore API; other APIs denied.
Use Case:: Enforce intra-namespace microservice boundaries.
Why it Matters:: Prevents accidental/malicious service calls to internal APIs.
Implementation Checklist:: podSelector for API pods; ingress from frontend label; restrict ports; `policyTypes: [Ingress]`.

=== Pattern: DENY All Traffic from Other Namespaces
[mermaid]
----
flowchart LR
  subgraph ns_foo ["namespace: foo"]
    FooPod[Any Pod]
  end
  subgraph ns_default ["namespace: default"]
    Web[app=web]
    DB[app=db]
  end
  subgraph ns_bar ["namespace: bar"]
    BarPod[Any Pod]
  end
  Web -.-> DB
  DB -.-> Web
  FooPod -. ❌ .-> Web
  FooPod -. ❌ .-> DB
  BarPod -. ❌ .-> Web
  BarPod -. ❌ .-> DB
----
Explanation:: Only internal namespace communication is permitted.
Use Case:: Tenant isolation; environment boundary.
Why it Matters:: Prevents privilege creep and meets audit separation requirements.
Implementation Checklist:: Policy selecting web & db; ingress limited to same-namespace (no namespaceSelectors) OR selective addition for trusted namespaces.

=== Pattern: ALLOW Traffic Only to a Metrics Port
[mermaid]
----
flowchart LR
  Prom[app=prometheus\\nrole=monitoring]
  subgraph API ["app=api"]
    Metrics[":5000 (metrics)"]
    HTTP[":8000 (http)"]
  end
  Prom -.-> Metrics
  Prom -. ❌ .-> HTTP
----
Explanation:: Prometheus may scrape metrics port; general HTTP port is blocked.
Use Case:: Observability access minimization.
Why it Matters:: Reduces exposure of non-observability endpoints to monitoring credentials.
Implementation Checklist:: Ingress from monitoring pods; allow port 5000 only.

=== Pattern: DENY External Egress Traffic
[mermaid]
----
flowchart LR
  subgraph ns_default ["namespace: default"]
    App1[app=web]
    App2[app=db]
  end
  External[External services / Internet]
  App1 -.-> App2
  App2 -.-> App1
  App1 -. ❌ .-> External
  App2 -. ❌ .-> External
----
Explanation:: Internal communication allowed; outbound to external networks denied.
Use Case:: Regulated workloads (PCI, OT) requiring strict egress control.
Why it Matters:: Prevents data exfiltration and command-and-control callbacks.
Implementation Checklist:: Egress policy; allow only explicit internal destinations (DNS, logging, etc.); `policyTypes: [Egress]`.

=== Pattern: DENY All Inbound to an Application (Except Specific Source)
[mermaid]
----
flowchart LR
  subgraph ns_default ["namespace: default"]
    Web[app=web]
  end
  subgraph ns_foo ["namespace: foo"]
    FooPod[Any Pod]
  end
  AnyOther[Any Pod]
  FooPod -.-> Web
  Web -.-> AnyOther
  Web -. ❌ .-> FooPod
  Web -. ❌ .-> AnyOther
----
Explanation:: Web can make outbound calls but only FooPod can reach it inbound.
Use Case:: Backend reachable only via controlled proxy or connector.
Why it Matters:: Prevents accidental exposure and narrows attack surface.
Implementation Checklist:: Policy selecting web; ingress rule permitting only proxy label; add `policyTypes: [Ingress,Egress]` if controlling both directions.

=== Operational Notes
* Selection Principle: Pods not selected by any policy remain open (all ingress/egress allowed). Once selected, only explicitly allowed traffic passes.
* Namespace Scope: Policies do not cross namespaces without `namespaceSelector`.
* Default Deny Strategy: Add an empty (or minimal) policy selecting pods to shift them into deny-by-default, then add granular policies.

=== Next Steps & Enhancements
Want YAML manifests and test harness? Provide preferred namespace & labels and we can generate ready-to-apply examples plus validation scripts (curl / netcat / exec loops).

=== Appendix: Validation Snippets
[source,sh]
----
# Test an allowed path
oc exec pod/frontend -- curl -s -o /dev/null -w '%{http_code}\n' http://api:8080

# Test a blocked path with timeout fallback
oc exec pod/untrusted -- curl -s --max-time 3 http://api:8080 || echo BLOCKED
----


