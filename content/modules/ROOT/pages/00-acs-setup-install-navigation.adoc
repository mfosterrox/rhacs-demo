= Lab Setup and Introduction
:toc:
:sectnums:
:icons: font

== Module Goals

* Access all applications in the lab environment
* Deploy default insecure applications
* Build and deploy a "Developer" image for the workshop

== Accessing the Workshop Components

Ensure you can access all necessary resources to complete this lab.

TIP: You can open URLs in a new tab. For example, on a Mac, use *<cmd> click*.

=== Access the Red Hat^(R)^ OpenShift Container Platform (OCP) UI

First, make sure you can access the Red Hat^(R)^ OCP console user interface.

*Procedure*

. Log into the OCP console at `{openshift_cluster_console_url}[{openshift_cluster_console_url}^]`.
. Enter the OCP credentials.
+
[cols="1,1"]
|===
| *User:* | {openshift_cluster_admin_username}
| *Password:* | {openshift_cluster_admin_username}
|===
+
image::01-ocp-login-password.png[OpenShift console login,link=self, window=blank, width=100%]
+
=== Access the Red Hat^(R)^ Advanced Cluster Security (RHACS) User Interface

Ensure that you have access to the RHACS user interface.

*Procedure*

. Log into the RHACS console at `{acs_route}[{acs_route}^]`.
+
image::01-rhacs-login.png[RHACS console,link=self, window=blank, width=100%]
+
. Enter the RHACS credentials:
+
[cols="1,1"]
|===
| *RHACS Console Username:* | {acs_portal_username}
| *RHACS Console Password:* | {acs_portal_password}
|===
+
image::01-rhacs-console-dashboard.png[RHACS console,link=self, window=blank, width=100%]


=== Access the Red Hat^(R)^ Quay UI

Red Hat Quay is an enterprise-grade container registry that provides a secure, scalable, and highly available solution for storing and managing container images. It offers integrated security scanning, access controls, and image signing to help ensure the integrity and security of containerized applications across hybrid and multi-cloud environments, including Red Hat OpenShift, AWS, Azure, and Google Cloud.

---

Let's ensure that you have access to the Quay User Interface.

*Procedure*

. Log into the Quay console at `{quay_console_url}[{quay_console_url}^]`
+
image::00-quay-login.png[Quay Console,link=self, window=blank, width=100%]
+
. Enter the Quay credentials:
+
[cols="1,1"]
|===
| *Quay Console Username:* | {quay_admin_username}
| *Quay Console Password:* | {quay_admin_password}
|===

=== OpenShift Admin Access Verification

Verify that you have admin-level access using the `oc` CLI. Run this in the bastion host terminal.

*Procedure*

. Switch to the admin context:
+
[source,sh,role=execute]
----
oc config use-context admin
----
+
. Verify access and check available nodes:
+
[source,sh,role=execute]
----
oc whoami
oc get nodes -A
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ oc whoami
oc get nodes -A
system:admin
NAME                            STATUS   ROLES                         AGE     VERSION
control-plane-cluster-dbv5h-1   Ready    control-plane,master,worker   7h17m   v1.33.6
worker-cluster-dbv5h-1          Ready    worker                        6h23m   v1.33.6
worker-cluster-dbv5h-2          Ready    worker                        6h23m   v1.33.6
----

You will now see the OCP role using the *oc* command, as we are currently working in the OpenShift cluster.

=== Verify the roxctl CLI and Access to RHACS Central Services

Next, verify that we have access to the RHACS Central Service.

*Procedure*

. Run the following command:
+
[NOTE]
====
This command uses variables saved in the ~/.bashrc file to authenticate with the RHACS Central Service.
====
+
[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" central whoami
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" central whoami
-bash: roxctl: command not found
----
+
. Install the roxctl CLI on the bastion and set the Central address. Run the following commands:
+
[source,sh,subs="attributes",role=execute]
----
mkdir -p ~/.local/bin
curl -L -f -o ~/.local/bin/roxctl "https://mirror.openshift.com/pub/rhacs/assets/latest/bin/Linux/roxctl"
chmod +x ~/.local/bin/roxctl
echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc
echo 'export ROX_CENTRAL_ADDRESS="$(oc -n stackrox get route central -o jsonpath='"'"'{.spec.host}'"'"')"' >> ~/.bashrc
source ~/.bashrc
roxctl
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
...
        TLS ServerName to use for SNI (if empty, derived from endpoint). Alternately, set the server name via the ROX_SERVER_NAME environment variable.

    --token-file:
        Use the API token in the provided file to authenticate. Alternatively, set the path via the ROX_API_TOKEN_FILE environment variable or set the token via the ROX_API_TOKEN environment
        variable.

    --use-current-k8s-context=false:
        Use the current kubeconfig context to connect to the central service via port-forwarding. Alternatively, set ROX_USE_KUBECONTEXT environment variable to true.

Usage:
  roxctl [command]
----
+
. Next, let's grab an API token and save a few variables for later use.
+
[source,sh,subs="attributes",role=execute]
----
export ROX_API_TOKEN=$(curl -sk -u "admin:{acs_portal_password}" "{acs_route}:443/v1/apitokens/generate" \
  -H 'Content-Type: application/json' \
  -d '{"name": "my-api-token", "role": "Admin"}' \
  | (jq -r .token 2>/dev/null || sed -n 's/.*"token":"\([^"]*\)".*/\1/p'))
echo $ROX_API_TOKEN
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ echo $ROX_API_TOKEN
YOUR_API_TOKEN_HERE
----
+
. Save the API token and a few other variables to the .bashrc file. These variables will stay in the .bashrc file so they're saved in case you need to refresh the terminal.
+
NOTE: Include `GRPC_ENFORCE_ALPN_ENABLED=false` so that roxctl gRPC calls (e.g. image scan) succeed when the TLS handshake does not advertise ALPN (avoids "missing selected ALPN property" errors with OpenShift routes).
+
[source,sh,subs="attributes",role=execute]
----
echo 'export ROX_API_TOKEN='"$ROX_API_TOKEN" >> ~/.bashrc
echo 'export GRPC_ENFORCE_ALPN_ENABLED=false' >> ~/.bashrc
echo 'export ACS_ROUTE="{acs_route}"' >> ~/.bashrc
echo 'export ACS_PORTAL_USERNAME="{acs_portal_username}"' >> ~/.bashrc
echo 'export ACS_PORTAL_PASSWORD="{acs_portal_password}"' >> ~/.bashrc
source ~/.bashrc
----
+
Verify the API token, and variables were saved correctly.
+
[source,sh,subs="attributes",role=execute]
----
cat ~/.bashrc
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
ROX_API_TOKEN="YOUR_API_TOKEN_HERE"
GRPC_ENFORCE_ALPN_ENABLED=false
ROX_CENTRAL_ADDRESS="{acs_route}"
ACS_PORTAL_USERNAME="{acs_portal_username}"
ACS_PORTAL_PASSWORD="{acs_portal_password}"
----
+
. Verify your roxctl access to the central service.
+
[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" central whoami
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
UserID:
        auth-token:265c6c62-7a74-4ee5-8a16-32bd01339a7a
User name:
        anonymous bearer token "my-api-token" with roles [Admin] (jti: 265c6c62-7a74-4ee5-8a16-32bd01339a7a, expires: 2027-02-21T22:57:19Z)
Roles:
        - Admin
Access:
        rw Access
        rw Administration
        rw Alert
        rw CVE
        rw Cluster
        rw Compliance
        rw Deployment
        rw DeploymentExtension
        rw Detection
        rw Image
        rw Integration
        rw K8sRole
        rw K8sRoleBinding
        rw K8sSubject
        rw Namespace
        rw NetworkGraph
        rw NetworkPolicy
        rw Node
        rw Secret
        rw ServiceAccount
        rw VirtualMachine
        rw VulnerabilityManagementApprovals
        rw VulnerabilityManagementRequests
        rw WatchedImage
        rw WorkflowAdministration
----
+
IMPORTANT: This output shows that you have unrestricted access to the RHACS product. These permissions can be seen in the **RHACS Access Control** tab that we will review later.
+
image::00-acs-access-control.png[RHACS access control,link=self, window=blank, width=100%]
+
*Great job!*

== Deploy the Vulnerable Workshop Applications

You now have access to the core OpenShift applications. Next, you'll deploy several insecure apps into the OpenShift cluster. Our insecure demo applications come from a variety of public GitHub repositories and sources. These apps will serve as the main use cases you examine during the workshop, ranging from well-known Capture the Flag (CTF) applications to vulnerabilities like Log4Shell and Apache Struts.

After deploying the applications, you'll scan some of these containers using the roxctl CLI to get a feel for what you'll be dealing with when you get to the security section with Red Hat Advanced Cluster Security.

=== Time to deploy

*Procedure*

. Run the following commands in the terminal, one after the other.
+
====
This command downloads a repository containing Dockerfiles, attack scripts, and Kubernetes manifests that you will use to deploy the containerized applications to OpenShift.
====
+
[source,sh,subs="attributes",role=execute]
----
git clone https://github.com/mfosterrox/demo-applications.git demo-applications
----
+
====
This command sets the variable TUTORIAL_HOME to equal the working directory, allowing you to make references to various files easily.
====
+
[source,sh,subs="attributes",role=execute]
----
cd ~/
echo export TUTORIAL_HOME="$(pwd)/demo-applications" >> ~/.bashrc
export TUTORIAL_HOME="$(pwd)/demo-applications"
----
+
====
This command applies the manifests to the OpenShift environment.
====
+  
[source,sh,subs="attributes",role=execute]
----
oc apply -f $TUTORIAL_HOME/k8s-deployment-manifests/ --recursive
oc apply -f $TUTORIAL_HOME/skupper-demo/ --recursive
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ oc apply -f $TUTORIAL_HOME/kubernetes-manifests/ --recursive
namespace/operations created
namespace/backend created
....
route.route.openshift.io/webgoat created
configmap/webgoat-config created
----
+
NOTE: You should see warnings such as: *Warning: would violate PodSecurity "baseline:latest": privileged (container "proxy" must not set securityContext.privileged=true)*. This is because you are deploying flawed container configurations and vulnerable container applications to the OpenShift cluster.
+
====
The following command will watch all of the applications as they are successfully deployed to the OpenShift cluster. You can press ctrl+c to terminate the command.
====
+
[source,bash,role="execute"]
----
oc get deployments -l demo=roadshow -A
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
NAMESPACE               NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
apache-struts           apache-struts           1/1     1            1           14m
backend                 api-server              1/1     1            1           13m
backend                 backend-atlas           1/1     1            1           13m
backend                 postgres                1/1     1            1           13m
backend                 varnish                 1/1     1            1           13m
dvwa                    dvwa                    1/1     1            1           13m
frontend                asset-cache             1/1     1            1           13m
frontend                monitor                 1/1     1            1           13m
frontend                tls-proxy               1/1     1            1           13m
frontend                wordpress               0/1     0            0           13m
frontend                wordpress-db            1/1     1            1           13m
juice-shop              juice-shop              1/1     1            1           13m
log4shell               log4shell               1/1     1            1           13m
medical                 patient-db              1/1     1            1           12m
medical                 proxy                   1/1     1            1           12m
medical                 reporting               1/1     1            1           12m
nodejs-goof-vuln-main   mongodb                 1/1     1            1           12m
nodejs-goof-vuln-main   nodejs-goof-vuln-main   1/1     1            1           12m
operations              jump-host               1/1     1            1           12m
operations              puppet-master           1/1     1            1           12m
payments                gateway                 1/1     1            1           12m
payments                mastercard-processor    1/1     1            1           12m
payments                visa-processor          1/1     1            1           12m
web-ctf-container       web-ctf-container       1/1     1            1           12m
webgoat                 webgoat                 1/1     1            1           12m
----
+
IMPORTANT: Please ensure the deployed applications are deployed and available before moving on to the next module.
+
. Run the following roxctl commands to check on the vulnerability status of the applications you deployed.
+
====
The following command triggers a vulnerability scan by RHACS, roxctl filters the results into a table. The severity flag means only the critical and important vulnerabilities will be shown. This image is known as the "Damn Vulnerable Web Application" and it contains a significant amount of vulnerabilities.
====
+
[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=quay.io/skupper/patient-portal-frontend:latest --severity CRITICAL,IMPORTANT --force -o table
----
+
TIP: The following output can be configured using flags. You can configure different outputs (table, CSV, JSON, and sarif) and filter for specific severities.
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=quay.io/skupper/patient-portal-frontend:latest --severity CRITICAL,IMPORTANT --force -o table
Scan results for image: quay.io/skupper/patient-portal-frontend:latest

+-------------+----------------------+----------------+-----------+------+---------------------------------------------------+----------------------+----------+---------------+
|    wheel    |        0.42.0        | CVE-2026-24049 | IMPORTANT | 7.1  | https://osv.dev/vulnerability/GHSA-8rrh-rw8j-w5fx |        0.46.2        |    -     |       -       |
+-------------+----------------------+----------------+-----------+------+---------------------------------------------------+----------------------+----------+---------------+
WARN:   A total of 15 unique vulnerabilities were found in 11 components
----
+
image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnY0NDA0ZnJqNXh6cGNqeHNxZGd5Zm5qMnlpOHhrbm1hY2pwcG5ydSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/p18ohAgD3H60LSoI1C/giphy.gif[link=self, window=blank, width=100%, class="center"]

[[build-a-container-image]]

== Build the main workshop application

Next, you are going to 

=== Login to Quay from the CLI

*Procedure*

. Run the following command.
+
====
Let's export a few variables to make things easier. These variables will stay in the .bashrc file so they're saved in case you need to refresh the terminal.
====
+
[source,sh,subs="attributes",role=execute]
----
echo export QUAY_USER={quay_admin_username} >> ~/.bashrc
QUAY_USER={quay_admin_username}
----
+
. Run the following command to set the Quay URL variable
+
[source,sh,subs="attributes",role=execute]
----
echo export QUAY_URL=$(oc -n quay get route quay-quay -o jsonpath='{.spec.host}') >> ~/.bashrc
QUAY_URL=$(oc -n quay get route quay-quay -o jsonpath='{.spec.host}')
----
+
IMPORTANT: Verify that the variables are correct
+
[source,sh,subs="attributes",role=execute]
----
echo $QUAY_USER
echo $QUAY_URL
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion ~]$ echo $QUAY_USER
echo $QUAY_URL
quayadmin
quay-bq65l.apps.cluster-bq65l.bq65l.sandbox209.opentlc.com
----
+
. Install Podman on the bastion host.
+
On RHEL, CentOS, Rocky Linux, Alma Linux, or Fedora:
+
[source,sh,subs="attributes",role=execute]
----
sudo dnf install -y podman
----
+
. Verify Podman is installed:
+
[source,sh,subs="attributes",role=execute]
----
podman --version
----
+
. Using the terminal on the bastion host, login to Quay using the Podman CLI as shown below:
+
[source,sh,subs="attributes",role=execute]
----
podman login $QUAY_URL -u $QUAY_USER -p {quay_admin_password}
----
+
console output:
+
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ podman login $QUAY_URL -u $QUAY_USER -p {quay_admin_password}
Username: quayadmin
Password:
Login Succeeded!
----
+
NOTE: Use the quay admin credentials to sign in if you run into issues.
+
[cols="2,2"]
|===
| *Quay Username:* | *{quay_admin_username}*
| *Quay Password:* | *{quay_admin_password}*
|===
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Username: quayadmin
Password:
Login Succeeded!
----

Beautiful! We're logged in. Now you can download, build, and push images directly to your Quay repository.

[[golden-image]]

=== Create our developer golden image

Imagine a base image as a blank canvas, providing a reliable foundation like a minimal operating system or runtime environment. Developers add the essential configurations, libraries, and tools needed to fit their application's unique demands. By tagging this customized creation as a golden image, developers ensure every deployment uses the same standardized, consistent setup, simplifying workflows and avoiding unexpected surprises.

*Procedure*

. Run the following commands in succession:
+
[source,sh,subs="attributes",role=execute]
----
podman pull python:3.12-alpine
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ podman pull python:3.12-alpine
Trying to pull docker.io/library/python:3.12-alpine...
Getting image source signatures
Copying blob e846654b130a skipped: already exists
Copying blob 00bb69fc5235 skipped: already exists
Copying blob 1f3e46996e29 skipped: already exists
Copying blob 7b104e645578 skipped: already exists
Copying config 451fdb03e1 done   |
Writing manifest to image destination
451fdb03e17285d3263d88f0f3bed692c82556935490d233096974386cd0381b
----
+
====
This command tags the python:3.12-alpine image with your Quay repository and username, allowing you to accurately push the image in a later step.
====
+
[source,sh,subs="attributes",role=execute]
----
podman tag docker.io/library/python:3.12-alpine $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1
----
+
====
This command shows you the local images available.
====
+
[source,sh,subs="attributes",role=execute]
----
podman images
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ podman images
REPOSITORY                                                                                  TAG          IMAGE ID      CREATED      SIZE
quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/python-alpine-golden  0.1          451fdb03e172  5 weeks ago  50.6 MB
docker.io/library/python                                                                    3.12-alpine  451fdb03e172  5 weeks ago  50.6 MB
----
+
====
This command pushes the golden image to your repository.
====
+
[source,sh,subs="attributes",role=execute]
----
podman push $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ podman push $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1
Getting image source signatures
Copying blob c6121fb0c7de done   |
Copying blob a0904247e36a done   |
Copying blob 5b6d0b88dd08 done   |
Copying blob 375bee7d34e2 done   |
Copying config 451fdb03e1 done   |
Writing manifest to image destination
----
+
Perfect!

[[dev-app]]

=== Create the Frontend Application and Upload It to Quay

In this section, you will add the application code to the image, build, tag, and push the new image to Quay. Later, we'll use that image to deploy an application to the OpenShift Cluster.

TIP: With the variables saved in the ~/.bashrc file, you will not have to declare them again in the future.

*Procedure*

. List the contents of the `frontend` directory inside the `image-builds` folder to see the files available for the project:
+
[source,sh,subs="attributes",role=execute]
----
ls $TUTORIAL_HOME/image-builds/frontend/
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ ls $TUTORIAL_HOME/image-builds/frontend/
Dockerfile  main.py  static
----
+
. Display the contents of the `Dockerfile` to understand how the image is built, including installed dependencies and the copied files:
+
[source,sh,subs="attributes",role=execute]
----
cat $TUTORIAL_HOME/image-builds/frontend/Dockerfile
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ cat $TUTORIAL_HOME/image-builds/frontend/Dockerfile
FROM python:3.12-alpine AS build

# Install Bash and other dependencies
RUN apk add --no-cache bash

# Install required Python packages
RUN pip install --no-cache-dir httpx psycopg psycopg_binary psycopg_pool starlette sse_starlette uvicorn

# Install vulnerable packages for testing
RUN pip install --no-cache-dir setuptools==39.1.0
RUN pip install --no-cache-dir Flask==0.5
RUN pip install --no-cache-dir Django==1.11.29
RUN pip install --no-cache-dir requests==2.19.0
RUN pip install --no-cache-dir PyYAML==3.12

FROM python:3.12-alpine AS run

# Install Bash in the runtime container
RUN apk add --no-cache bash

# Create user and set permissions
RUN adduser -S fritz -G root
USER fritz

# Copy files from the build stage
COPY --from=build /usr/local/lib/python3.12/site-packages /usr/local/lib/python3.12/site-packages
COPY --chown=fritz:root static /home/fritz/static
COPY --chown=fritz:root main.py /home/fritz/main.py

# Expose the port and set the working directory
EXPOSE 8080
WORKDIR /home/fritz

# Set the entry point to start the application
ENTRYPOINT ["python", "main.py"]
----
+
. Update the `FROM` statement in the Dockerfile to reference a custom base image hosted in a private registry, using `sed` to modify the line:
+
[source,sh,subs="attributes",role=execute]
----
sed -i "s|^FROM python:3\.12-alpine AS \(\w\+\)|FROM $QUAY_URL/$QUAY_USER/python-alpine-golden:0.1 AS \1|" $TUTORIAL_HOME/image-builds/frontend/Dockerfile
----
+
. Check the Dockerfile again to verify that the `FROM` statement has been updated correctly:
+
[source,sh,subs="attributes",role=execute]
----
cat $TUTORIAL_HOME/image-builds/frontend/Dockerfile
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion ~]$ cat $TUTORIAL_HOME/image-builds/frontend/Dockerfile
FROM quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/python-alpine-golden:0.1 AS build

RUN pip install --no-cache-dir

....
----
+
. Build the Docker image using `podman` from the `frontend` directory. The `-t` flag tags the image with a version (`0.1`) and a registry URL:
+
[source,sh,subs="attributes",role=execute]
----
cd $TUTORIAL_HOME/image-builds/frontend/
podman build -t $QUAY_URL/$QUAY_USER/frontend:0.1 .
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion frontend]$ podman build -t $QUAY_URL/$QUAY_USER/frontend:0.1 .
[1/2] STEP 1/2: FROM quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/python-alpine-golden:0.1 AS build
.
.
.
Successfully tagged quay-w2t4c.apps.cluster-w2t4c.w2t4c.sandbox1647.opentlc.com/quayadmin/frontend:0.1
46ea42cba3f17c366b0c534164ae088719266df9ab4122532b0bffd1bbefaec9
----
+
. Upload the built image to a remote registry using `podman push`. The `--remove-signatures` flag ensures that image signatures are not included in the pushed image:
+
[source,sh,subs="attributes",role=execute]
----
podman push $QUAY_URL/$QUAY_USER/frontend:0.1 --remove-signatures
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[demo-user@bastion frontend]$ podman push $QUAY_URL/$QUAY_USER/frontend:0.1 --remove-signatures
Copying blob e2adcecab318 done   |
Copying blob 9944062081bf done   |
Copying blob 83e98ac5789e skipped: already exists
Copying blob 32fd82e104c5 skipped: already exists
Copying blob 453d5d1264c7 done   |
Copying blob 3c37dc31320d done   |
Copying blob 210a2ae1a75e skipped: already exists
Copying blob 57a6ec527341 skipped: already exists
Copying config 46ea42cba3 done   |
Writing manifest to image destination
----
+
NOTE: Quay will automatically create a private registry to store the application image because of your admin access.
+
The frontend application is finally built! The next step is to review it in Quay and deploy it to OpenShift.

[[quay]]

== Red Hat Quay

Red Hat Quay is an enterprise-quality registry for building, securing, and serving container images. It provides secure storage, distribution, and governance of containers and cloud-native artifacts on any infrastructure.

=== Accessing Quay

Your Red Hat Quay console is available at: {quay_console_url}[window=blank]

Administrator login is available with:

[source,sh,subs="attributes",role=execute]

[cols="1,1"]
|===
| *Quay Console Username:* | {quay_admin_username}
| *Quay Console Password:* | {quay_admin_password}
|===

[[navigating-the-registry]]

=== Browse the Registry

In the setup module, you downloaded, built, and pushed an insecure Java application called *frontend*. Now it's time to deploy it to the OpenShift Cluster. To do this, you will need to make the registry that you created public.

Let's take a look at our application in the registry.

*Procedure*

. First, click on the *frontend* repository.
+
image::00-quay-login.png[link=self, window=blank, width=100%]
+
The information tab shows you information such as:
+
- Podman and Docker commands
- Repository activity
- The repository description
+
image::00-frontend-repo.png[link=self, window=blank, width=100%]
+
====
On the left-hand side of the window, you should see the following icons labeled in order from top to bottom:
====
+
image::00-quay-sidebar.png[link=self, window=blank, width=100%]

- Information
- Tags
- Tag History
- Usage Logs
- Settings
+
. Click on the *Tags* icon.
+
image::00-quay-tags.png[link=self, window=blank, width=100%]
+
This tab displays all of the images and tags that have been uploaded, providing information such as fixable vulnerabilities, the image size, and allows for bulk changes to images based on the security posture.
+
We will explore this tab a little later in this module.
+
. Click on the *Tags History* icon.
+
image::00-quay-history.png[link=self, window=blank, width=100%]
+
This tab simply displays the container image history over time.
+
. Click into the SHA256 hash number.
+
image::00-image-details.png[link=self, window=blank, width=100%]
+
From this dashboard, you will be able to see the image manifest of that container image.
+
image::00-image-manifest.png[link=self, window=blank, width=100%]
+
. Click the *BACK* icon in the top left of the dashboard then click on the *Usage Logs* icon.
+
image::00-quay-back.png[link=self, window=blank, width=100%]
image::00-quay-usage.png[link=self, window=blank, width=100%]
+
This tab displays the usage over time along with details about who/how the images were pushed to the cluster.
+
====
You should see that you (the "quayadmin") pushed an image tagged 0.1 to the repository today.
====
+
. Lastly click on the *Settings* icon.
+
image::00-quay-settings.png[link=self, window=blank, width=100%]
+
In this tab, you can add/remove users and update permissions, alter the privacy of the repository, and even schedule alerts based on found vulnerabilities.
+
. Make your repository public before deploying our application in the next step by clicking the *Make Public* button under *Repository Visibility*:
+
image::00-quay-public.png[link=self, window=blank, width=100%]
+
. Click OK:
+
image::00-quay-public-yes.png[link=self, window=blank, width=100%]

[[vulnerability-scanning-with-quay]]

IMPORTANT: You will need to make your repository public before deploying our application in the next step by clicking the *Make Public* button under *Repository Visibility*.


=== Vulnerability Scanning with Quay

Red Hat Quay can also help with securing our environments by performing a container vulnerability scan on any images added to our registry and advising which ones are potentially fixable. This feature is also known as vulnerability scanning at rest.

Use the following procedure to check the security scan results for our Java container image you have uploaded.

*Procedure*

. Click on the *Tags* icon on the left side of the screen like before.
+
image::00-quay-tags.png[link=self, window=blank, width=100%]
+
NOTE: You may need to click the checkbox near the image you would like more information on, but the column for *Security Scan* should populate.
+
. By default, the security scan color codes the vulnerabilities, you can hover over the security scan for more information.
+
image::00-quay-scan-hover.png[link=self, window=blank, width=100%]
+
NOTE: The Alpine container image you are using in this lab shows 34 vulnerabilities, with 2 critical vulnerabilities. This number will change with time and will be different between container scanners for a variety of reasons such as reporting mechanisms, vulnerability feeds, and operating system support.
+
. Click on the list of vulnerabilities to see a more detailed view.
+
image::00-quay-vuln-overview.png[link=self, window=blank, width=100%]
+
. Click on a vulnerable package on the left menu to get more information about the vulnerability and see what you have to do to fix the issue.
+
image::00-quay-vuln-detailed.png[link=self, window=blank, width=100%]
+
NOTE: Toggling for fixable/unfixable vulnerabilities is an excellent way for developers to understand what is within their responsibility for fixing. For example, since you are using an older version of Java, many fixes are available for these common issues.

=== Check if RHACS Can Pull the Image Manifest from Your Quay Instance

*Procedure*

Run the following command to test the RHACS/Quay integration:

[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=$QUAY_URL/$QUAY_USER/frontend:0.1 --force -o table
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
ERROR:  Scanning image failed: retrieving image scan result: could not scan image: "quay.apps.cluster-p2z2g.dynamic.redhatworkshops.io/quayadmin/frontend:0.1": rpc error: code = Canceled desc = context canceled. Retrying after 3 seconds...
----

====
Why would this error occur?
====

. Add the Quay registry integration to RHACSvia API so ACS can pull the image manifest.
+
[source,sh,subs="attributes",role=execute]
----
curl -sk -X POST "https://${ROX_CENTRAL_ADDRESS}:443/v1/imageintegrations" \
  -H "Authorization: Bearer ${ROX_API_TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Quay Workshop Registry",
    "type": "quay",
    "categories": ["REGISTRY"],
    "quay": {
      "endpoint": "'"$QUAY_URL"'",
      "insecure": true
    }
  }'
----
+
TIP: The following output can be configured using flags. You can configure different outputs (table, CSV, JSON, and sarif) and filter for specific severities.
+
. Run the command again to verify the integration was successful.
+
[source,sh,subs="attributes",role=execute]
----
roxctl --insecure-skip-tls-verify -e "$ROX_CENTRAL_ADDRESS:443" image scan --image=$QUAY_URL/$QUAY_USER/frontend:0.1 --force -o table
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
.
|    pip     | 25.0.1  |  CVE-2025-8869   | MODERATE  |  0   |  https://nvd.nist.gov/vuln/detail/CVE-2025-8869   |     25.3      |    -     |       -       |
|            |         +------------------+-----------+------+---------------------------------------------------+---------------+----------+---------------+
|            |         |  CVE-2026-1703   |    LOW    |  0   |  https://nvd.nist.gov/vuln/detail/CVE-2026-1703   |     26.0      |    -     |       -       |
+------------+---------+------------------+-----------+------+---------------------------------------------------+---------------+----------+---------------+
WARN:   A total of 33 unique vulnerabilities were found in 8 components
----

=== Deploy Your Newly Built Application

The last step is to deploy the Skupper application with your specific frontend container. You will do this by altering an existing manifest.

*Procedure*

. Run the following commands in the terminal, one after the other:

====
This command sets new variables for the Skupper example.
====

[source,sh,subs="attributes",role=execute]
----
echo export APP_HOME="/home/lab-user/demo-applications/skupper-demo" >> ~/.bashrc
export APP_HOME="/home/lab-user/demo-applications/skupper-demo"
----

====
Next, swap the default image repo for the local Quay one containing the frontend app you built.
====

[source,sh,subs="attributes",role=execute]
----
sed -i "s|quay.io/skupper/patient-portal-frontend:latest|$QUAY_URL/$QUAY_USER/frontend:0.1|g" $APP_HOME/frontend.yml
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion skupper-demo]$ sed -i "s|quay.io/mfoster/frontend:latest|$QUAY_URL/$QUAY_USER/frontend:0.1|g" $APP_HOME/frontend.yml
----

====
Verify the swap was successful:
====

[source,sh,subs="attributes",role=execute]
----
cat $APP_HOME/frontend.yml
----

[.console-output]
[source,yaml,subs="+macros,+attributes"]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: frontend
    demo: roadshow
  name: frontend
  namespace: patient-portal
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - name: frontend
          image: quay-h2xfr.apps.cluster-h2xfr.h2xfr.sandbox2437.opentlc.com/quayadmin/frontend:0.1

		  (For example: Yours will be different)

          imagePullPolicy: Always
          env:
----

====
Next, deploy the application with your updated manifests:
====

[source,sh,subs="attributes",role=execute]
----
oc apply -f $APP_HOME/frontend.yml
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
namespace/patient-portal created
deployment.apps/database created
service/database created
deployment.apps/frontend created
route.route.openshift.io/frontend-patient-route created
service/frontend-service created
deployment.apps/payment-processor created
service/payment-service created
----

====
And lastly, verify that the application is running:
====

[source,sh,subs="attributes",role=execute]
----
oc get pods -n patient-portal
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
[lab-user@bastion skupper-demo]$ oc get pods -n patient-portal
NAME                                 READY   STATUS    RESTARTS   AGE
database-758f8f75f7-46ndf            1/1     Running   0          56s
frontend-5c487dcb47-7gsm7            1/1     Running   0          54s
frontend-5c487dcb47-f25hm            1/1     Running   0          54s
frontend-5c487dcb47-z9stn            1/1     Running   0          54s
payment-processor-6644dfc5b7-5vzqh   1/1     Running   0          51s
payment-processor-6644dfc5b7-96jb6   1/1     Running   0          51s
payment-processor-6644dfc5b7-9vl7b   1/1     Running   0          51s
----

== Summary

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnY0NDA0ZnJqNXh6cGNqeHNxZGd5Zm5qMnlpOHhrbm1hY2pwcG5ydSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/p18ohAgD3H60LSoI1C/giphy.gif[link=self, window=blank, width=100%, class="center"]

*Great Job!*

Now that the setup is all done, you are ready to move on with RHACS and dive into the security considerations.

On to *Visibility and Navigation*!
